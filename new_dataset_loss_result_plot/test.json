{"Technology": {"precision": 0.9259, "recall": 0.8065, "fscore": 0.8621, "auc": 0.8548, "acc": 0.7714}, "Military": {"precision": 1.0, "recall": 1.0, "fscore": 1.0, "auc": 1.0, "acc": 1.0}, "Education and Test": {"precision": 0.0, "recall": 0.0, "fscore": 0.0, "auc": 0, "acc": NaN}, "Disaster incident": {"precision": 0.9091, "recall": 0.7407, "fscore": 0.8163, "auc": 0.7778, "acc": 0.7429}, "Politics": {"precision": 0.8, "recall": 0.5614, "fscore": 0.6598, "auc": 0.7327, "acc": 0.6667}, "Medicine and Health": {"precision": 0.8861, "recall": 0.814, "fscore": 0.8485, "auc": 0.9013, "acc": 0.8062}, "Financial Business": {"precision": 0.5, "recall": 0.2, "fscore": 0.2857, "auc": 0.5714, "acc": 0.5833}, "Entertainment": {"precision": 0.8739, "recall": 0.7544, "fscore": 0.8098, "auc": 0.8477, "acc": 0.7569}, "Social Life": {"precision": 0.0, "recall": 0.0, "fscore": 0.0, "auc": 0, "acc": 0.0}, "auc": 0.8443737896618501, "metric": 0.7359213034968377, "recall": 0.7582737474551865, "precision": 0.7305257894875343, "acc": 0.7546403712296984, "legitimate_accuracy": 0.7482993197278912, "fake_accuracy": 0.7682481751824818}